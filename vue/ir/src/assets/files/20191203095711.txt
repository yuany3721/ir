智能建筑与城市信息
绿色数据中心
陈亮
摘要：绿色数据中心是指在数据中心的全生命周期内，最大限度地节约资源，保护环境并减少污染，为人们提供可靠、安全、高效、适用的、与自然和谐共生的信息系统使用环境。可持续发展是企业的命脉，许多数据中心客户正在积极寻求各种方法节省能源，降低成本，保证企业不断发展，因此，绿色数据中心渐渐成为企业在建设或改进数据中心时的一个重要方向。　
关键词：数据中心；智慧企业；战略；标准
一、绿色数据中心是智慧企业的选择
全球化经济加剧了竞争，也促进了合作，企业为在不断变化的环境下求得生存，必须做到可持续发展。绿色意识可以转化为绿色措施。能效规划和企业责任可以取得积极的结果。尽量采用绿色技术，同时降低成本和风险是企业的必经之路。
面对数据中心用电量、制冷能力和空间已达到或接近极限，数据中心对环境的影响日益引起社会关注，从企业运营成本和可持续发展的角度考虑，建立绿色数据中心势在必行，是企业减少风险、树立良好形象的必由之路。
（1）管理不断增长的能源成本
据国际正常运行时间协会UptimeInstitute调查，目前服务器三年用电和冷却的费用一般为服务器硬件采购成本的1.5倍。随着经济型、功能更加强大的高性能计算机集群需求的增长，电力成本还将不断上升，而且关系到预算能否承受电力和冷却费用的问题。
（2）电力不够
由于城市的高速发展，区域电力系统逐渐无法满足日益增长的扩容需求。然而，新的服务器、存储和网络产品在性能提高、价格降低的同时，耗电量却在不断增大。因此，电力不够成为企业面临的棘手问题。
（3）冷却能力不够
目前，许多客户的数据中心已用了10~15年，冷却基础设施难以满足当前需求。传统冷却方法可为每机架提供2~3kW制冷量，而目前每机架需要的制冷量却达到了20~30kW。单机柜的功率密度比数据中心过去的设计指标提高了许多倍。
（4）空间不够
每当新项目或应用上线时，需要配置新的服务器或存储子系统，因此，随着业务需求的增长，设备占地面积的激增，当不能添加服务器和存储时，只好再建一个数据中心，这种扩建的成本非常高。
上述问题的不断出现，表明建立绿色数据中心是企业的必然选择。
1.2数据中心能源使用比例分析
传统数据中心的能源使用情况，每种构成分为两部分：
（1）IT设备（服务器、存储和网络）使用45%的能源；支持这种设计的基础设施使用另外55%的能源，如制冷机组、加湿器、计算机房和空调（CRAC）、配电箱（PDU）、不间断电源（UPS）、配电系统等。
（2）处理器仅使用30%的能源，而系统其余部分则使用了剩余的70%。
（3）服务器的利用率一般仅为20%，而剩余的80%都是闲置的。
由此可以看出，对于无效使用能耗的降低、高效硬件的使用和IT资源的高使用率对于降低企业运营成本、提升可持续发展来说十分重要。
1.3绿色数据中心的标准
UptimeInstitute白皮书定义了四个确定数据中心相对“绿色”的要素。四个绿色指标是IT系统设计和建筑、IT硬件资产利用、IT硬件效率和机房物理基础设施。
针对绿色数据中心，目前尚没有专用的评估体系，实践中通常采用绿色建筑的评估体系来衡量绿色数据中心。
为了评估数据中心绿色环保水平，经常使用以下两个指标：
（1）数据中心基础设施效率(DCiE)，DCiE=（IT设备电量/基础设施总电量）x100%。
（2）电源使用效率（PUE），PUE=基础设施总电量/IT设备电量。
IT设备用电量包括所有IT设备以及用于监控或控制数据中心的辅助设备的负荷，前者如服务器、存储和网络设备；后者如键盘、视频、鼠标开关、监视器、工作站或移动计算机。
基础设施用电总量包括IT设备及支持IT设备的系统负荷，如：
◆供电设备，如不间断电源（UPS）、开关柜、发电机、配电箱（PDU）、电池、IT设备外部配电损耗；
◆冷却系统，如制冷机组、计算机房空调（CRAC）、直接膨胀空气调节器（DX）、泵及冷却塔等；
◆计算机、网络及存储；
◆低负载条件下工作时，不间断电源（UPS）设备效率下降；
◆其他杂项器件的负载，如数据中心照明。例如：DCiE值为33%（PUE等效值为3.0）时，表明IT设备耗用数据中心电量的33%。因此，支付100元能源费，IT设备实际只用了33元。
二、实现绿色数据中心的关键战略
2.1绿色数据中心的要素
为实现绿色数据中心，需考虑三个因素：
（1）基础设施
机房基础设施方面需要考虑的问题包括：
◆如何以及在何处使用能源；
◆基础设施目前可优化的空间（从DCiE或PUE来衡量）；
◆本数据中心是以电量和性能为主，还是仅以性能为主；
◆是投资建立新的数据中心，还是投资升级现有数据中心；
◆数据中心现场是否适应变更；
◆数据中心所需要的可靠性水平是否增加了基础设施能耗，现有备份或备用设备存在多少闲置，是否足够，还是过多，能否撤销部分设备；
◆应选择哪些支持设备（不间断电源、飞轮发电机、发电机、配电柜、制冷机组、CRAC等），今后的发展趋势如何，基础设施能否满足下一代硬件电力和冷却要求，例如，更多的IT设备今后采用水冷；
◆基础设施是否存在过热问题，是否存在湿度问题；
◆是否可以采用免费制冷；
◆电力、冷却或空间是否影响当前运营，哪些因素影响今后业务发展，未来能否在现有能源范围内增加计算能力；现场基础设施在以下方面是否达到最佳水平：气流与散热、配电分配、冷却、照明、监控与管理；
◆必要时，是否需要采用水冷。
（2）IT设备
IT设备方面的问题如下，其中包括硬件设计以及机架现有冷却、供电和监控方式：
◆设备是否采用节能硬件，是否采用节电功能；
◆目前是按现场、基础设施还是机架选择供电和冷却方法；
◆硬件是否具备电量、热量、资源利用率监控功能，是否可以监控能耗；
◆用电量如何计费；
◆谁可以提供帮助。
（3）利用率
服务器和存储利用率方面的问题如下：
◆基础设施利用率是否达到最佳水平；
◆是否存在不必要的备份设备；
◆可以进行合并与虚拟化吗；
◆如何将离散或孤岛式计算转变为共享模式；
◆是否可以监控资源利用率，当前情况及未来趋势如何；
◆如何对基础设施提供的服务进行计费；
◆谁能提供帮助。
2.2基础设施的绿色技术
为实现环保，数据中心需要采用高能效基础设施和最佳实践措施。基础设施寿命比大部分IT硬件长三到五倍，所需投资也更高。
因此，了解基础设施更新的时间安排很重要。
本节还将提供采用最佳实践措施提高基础设施效率的技巧。
机房与设备基础设施可分为以下几部分，这几个部分之间彼此相关：
◆数据中心建筑结构；
◆数据中心绿色能源；
◆数据中心冷却；
◆供暖、通风与空调（HVAC）；
◆不间断电源（UPS）；
◆电源；
◆备用发电机或替代电源。
2.2.1建筑结构绿色技术
绿色节能建筑结构新体系是在木结构、砌体结构、框架结构等通常结构体系的基础上产生和发展的，具有绿色节能房屋建筑结构特点的新体系。所采用的墙体、楼板等主要建筑结构材料具有绿色、节能性能，而主体建筑结构和施工方式采取现场拼装、现场浇筑使建筑物形成一体化，是一种将建筑材料、结构形式和施工方式相结合的以体现绿色、节能为主的新的建筑结构体系。
建筑结构的节能技术有以下几种：
（1)优化建筑外形，以最小的建筑外表面积创造出最大的建筑空间，减轻墙外表面积对散热的影响。合理设计平面，将电梯、楼梯、管道井、机房等布置在建筑物的西侧，可以有效阻挡日射，降低夏季室内的温度。
（2)采用保温隔热、传热系数小的墙体新材料。
（3）外装饰尽量做浅色处理，采用光滑饰面材料，以减少表面对辐射的吸收。
（4）利用绿色植被铺地，将降低地面附近室外空气的温度，减少室内外温差，从而降低冷负荷。
（5）屋顶采用高保温材料或在屋顶设置高效隔热层，防止大量辐射热侵入室内，减少空调等耗能。条件允许时可在屋顶上覆土，做植被屋面，不仅能用覆土隔热还能利用植物遮阳。
2.2.2绿色能源
新一代数据中心应充分利用太阳能、风能、水力能、生物能、海洋能和地热能等绿色能源，实现一定程度上的资源自给自足，结合各地的自然条件及资源情况，因地制宜地开发绿色能源。如太阳能热水技术、沼气技术、地热发电、地热供暖技术、被动式太阳能利用技术、可再生技术等。
数据中心采用创新技术可以提高单位电耗计算能力。随着技术的演进和创新，IT设备能效不断提高，采用新型设备更换原有设备能够显著降低数据中心的总电耗和制冷要求，节省宝贵的地面空间，通常会带来较好的投资回报率。
例如刀片服务器的耗电量和制冷要求比1U[SG1]技术降低了25%~40%；最新型UPS系统的耗电量比现有UPS系统低70%；新型制冷机组的系统效率提高了50%；新型制冷机组可以通过安装变速驱动器，降低泵水系统的能耗，并且便于这种水冷系统与冷媒水基础设施更好地集成；利用外部空气直接对冷媒水进行降温的水侧节能装置，可进一步降低冷却数据中心使用的能耗；利用储热系统保存制冷机组通常夜间高效工作时产生的能量，然后在能源成本较高的白天释放这种能量，可以降低水冷系统的运行成本。
2.2.3改善冷却方式
制定减少数据中心产生的热量、提高电源和制冷效率的计划时，应考虑大量因素。
（1）改进机架和机房布局可以提高能效，且初期投资比较低。可考虑以下改进方法：
◆IT设备按热通道和冷通道配置排列；
◆设备应位于可以控制冷、热通道之间气流的位置，避免热空气回流到IT设备的冷风进气口；
◆采用辅助冷却方式，如水冷或冷媒热交换器。
采用后门热量交换器提高机架制冷效率，或采用封闭式机架系统在进入机房之前驱散高密度计算机系统产生的热量。同样，相对简单的气流管理可以显著提高能效。例如：清除地板下面的障碍，有效管理线缆以利于空气流通；通过增加或减少设备进气口的出风口盖板，保证地板孔与设备热负荷相匹配；考虑增加回风管。
应考虑数据中心组成热区，将一组固定的IT设备和地面空间分配给指定的HVAC或CRAC设备。这种空间和散热计划可以消除机房中对制冷系统形成压力的热区（热点），提高系统稳定性，避免由于热引起硬件故障，同时，也可以避免冷点。
（2）管理气流
在可能的情况下，应避免冷、热空气相混。为提高气流效率，冷空气穿过高架地板下面进入负载区域的通道必须通畅。高架地板上面，应有热空气返回CRAC设备的通道。以下方法可以改进气流管理：
◆采用冷热通道。采用冷、热通道配置便于更好地管理高架地板气流，包括热空气和冷空气。这种配置有助于冷、热气流在各自独立的通道中流动，减少空气混合，提高效率；必要时可以隔离冷热通道，以提高效率；
◆增加或减少出风口盖板。减少热通道和开孔区域的出风口盖板；增加高热负荷区的通风口盖板；调节通风口盖板下面的挡板，使少量空气进入低热区，高热区处挡板全部打开；开口未用的盖板用整块盖板更换；
◆改善机架气流。可能的情况下，避免冷、热空气相混。空置机架安装隔板，机架间留大间距，使冷空气避开服务器产生的热负荷，并使冷空气流回CRAC[SG2]设备。空气应以最小的阻力穿过通道，这样可以使机架中的热空气再次穿过服务器被带走；BladeCenter（刀片中心）机箱后部未使用的模块托架安装相应的隔板、填充物；
◆密封线缆口和透孔。高架地板开口会影响气流分布，降低地板下面的静气压，用填充物、泡沫、枕垫封堵开口，这样可以使更多的空气进入需要的位置；
◆清除地板下的障碍。地板下障碍过多会导致静压上升，高静压对高架地板上、下的气流产生负面影响。清除地板下面的障碍物，如：不使用的线缆和布线、不使用的地板下设备和通信盒。
（3）高架地板高度建议
目前，高架地板建模建议高度至少支持600mm（24in）无障碍空间，为冷空气提供通畅的流动通道。有些新的高架地板为900mm（36in）高，以便增加空气量，满足极为严格的冷却要求。
对于低高架地板，如300mm（12in），设备不得靠近CRAC系统，否则会造成出风口盖板出现低气流或逆流。地板下面部分可放置在高架地板下，使空气进入所需的区域。[SG3]
（4）数据中心密封
考虑以下数据中心密封方案：
◆隔离数据中心墙壁与天花板；
◆密封数据中心四周的透孔；
◆用双层玻璃窗更换窗户；
◆安装门口密封条。
这些简单的方法有效地保持数据中心温度和湿度设置点，提高热效率。
（5）制冷设备定位方案
传统IT设备冷却设计将多个CRAC系统放在数据中心四周，CRAC设备提供的冷空气从设备到服务器机架要经过一段距离，然后再返回CRAC设备。
为解决数据中心高密度机架产生的热点问题，制冷设备厂商开始提供另一种冷却解决方案消除数据中心的热点。其做法是在靠近出现问题的位置，放置热交换器进行局部液冷，将热交换器直接放在热源处，不必使用CRAC系统，这样可以提高其余CRAC系统的效率，保证数据中内部的冷却能力。这些热交换器均可以扩展。
为发挥局部冷却的优势，数据中心需要配备冷媒水。数据中心内部提供冷媒水不是新技术，20世纪80年代已用于大型机。由于风冷不能满足高密度服务器的散热需求，于是再次要求数据中心配备冷媒水。目前，已基于这一策略为机架开发出几种可行方案：
◆前部或后部安装鳍管热交换器；
◆机架底部或侧面安装内部鳍管热交换器；
◆高架鳍管热交换器；
◆服务器内部制冷；
◆后门热量交换器。
2.2.4供暖通风与空调（HVAC）
（1）自然冷却免费制冷
节能装置分为空气侧和水侧两种。空气侧节能装置可用作免费制冷系统。不过，根据所处位置，这些装置在有持续鲜冷气源的条件下效果最好。通过夜间对这些装置进行调整可以保持一致性。外部空气节
能装置可直接抽取外部空气供数据中心使用。水侧节能装置采用户外冷空气生成冷凝水，可用于部分或全部满足设备冷却的要求。
当外部气温足够低时，水侧节能装置可部分或完全取代制冷机组，这样可以延长每天免费制冷的时间。
（2）温度设定
数据中心温度设定点只要上调1度，就可显著降低能源成本，因为，这样可以降低CRAC负荷，更多地采用免费制冷。
建立能源监测与管理系统，可监控、管理各系统用电量和热量，正确评估数据中心高温和低温设定点并可以提供趋势分析，有助于控制能耗，提高能源利用率，节省能源
费用。　
2.2.5电源
现场电源功率因校正（PFC）可以重新获得部分损失的电量。通过PFC，供电1kW，设备使用的电量可以达到0.95kW。对于使用2500~3000kW的现场，回报期为三到四年。
智能电源分配单元（iPDU）连接能源管理系统，通过收集用电量信息，显示服务器能耗的整体视图。先进的能源管理系统，可以实现对连接的服务器进行功率封顶，节省费用，有助于提高能效。
（1）DC与AC
随着交流电源的发展和开关电源技术的进步，效率方面的提高达到空前水平。这种情况加大了DC电源与AC电源之间的差距，DC效率的提高仅为5%~7%，而AC产品则十分丰富，成本大大低于DC。
（2）飞轮发电技术
随着新的飞轮发电技术的出现，过去的UPS电池有被新的飞轮发电机取代的趋势。受电池质量和充电次数的影响，某些情况下，电池的使用寿命最多为10年，而与电池相比，飞轮发电机可以在更高的温度下工作，效率高，占地面积比电池小，不必进行交流到直流，再到交流的转换，但是它支持的时间却有限，也不能调节电源。飞轮发电机的优点是效率高、尺寸小；缺点是支持时间有限，不能调节电源。
（3）发电机
电源保障是实现高可用性数据中心的关键，而备用发电机是高可用性数据中心现场基础设施的关键部件。UPS系统可以为数据中心供电几分钟，甚至几个小时，但是没有备用发电机，数据中心制冷的HVAC系统无法工作，从而有可能造成数据中心温度过高。
目前，与老式发电机相比，备用发电机在设计上显著提高了燃油效率，减少了对大气的CO2排放，更加环保。同时，加快了启动和送电速度（低于30秒）支持使用飞轮和燃料电池技术的备用发电机可以取代UPS电池。
备用发电机一般采用柴油或天然气为燃料，这些设备的使用寿命为15~20年。电力供应非常好的地区，这些设备未必很长时间工作，因此保养和检测是十分重要的。确定这些设备的规格时，一定要考虑含
数据中心在内的全部基础设施，包括制冷机组、冷却泵、CRAC、UPS、AHU及其他现场基础设施。
除此之外，现场发电还在不断发展的新技术有：燃料电池、核能、风能和太阳能等。
2.3IT绿色技术
优化IT设备，从源头减少用电量和产生的热量，对提高基础设施效率可以产生直接影响。
（1）如何用电
电进入数据中心现场后，配送给各种IT组件，系统中的电力分配取决于其架构和用途。供电总量和组件配电量随处理的工作负载而变化，并且，每种系统都根据特定用途而构建。
图4中从左至右显示的电力分配情况，依次为大型机系统、高端UNIX服务器、高性能计算（HPC）服务器、入门级UNIX系统和刀片系统。由于处理器能耗约占大型机的20%~30%，而刀片为50%以上，因此我们在优化能效时，需要分别针对每种系统采用不同的策略。柱状图底部红色部分表示每种系统所需能量，即交流电转换为直流电损失部分能量。变压器（电源）效率取决于负载，是非线性的，最有效的负载为50%~75%。负载低于50%，效率显著下降；因此尽管不能显著提高效率，也要保持在一定的负载水平。
（2）创新解决IT硬件散热问题
为防止计算机芯片损坏，芯片及整个系统必须进行散热。
系统每节省1W电，热负荷也可以节省约1W，这些节省还可对不间断电源（UPS）和制冷系统产生同样的效果。因此，降低系统能耗可以取得两倍以上的回报，这是实现绿色数据中心的一大优点。
空气是一种效率极低的冷媒，因此液冷成为一种越来越流行的冷却方法。水是目前最常用的冷却液体，一升水吸收的热量约比同样体积的空气高4000倍。随着空间减小，热量增加，水冷系统将很快成为一种必然趋势。在规划新的数据中心或改造现有中心时，应考虑新IT设备需要采用水冷提高散热效率的因素。
例如：目前已开发出在芯片背部直接水冷的解决方案，其基本原理是让水流穿过芯片背部的细微通道。热能被水吸收后，可以达到有效散热的目的，典型的如支持高性能计算的POWER575系统就配有水冷处理器。
2.4提高数据中心资源利用率的绿色技术
选择高效设备，采用更高效的系统可以提升数据中心资源利用率，典型的有合并和虚拟化技术。
（1）合并：提高能效的关键合并的概念如图5所示。假定我们有四个系统，每个系统运行两个应用程序（APP）。同时，每个设备耗电量为2kW，总计8kW。而就小型x86服务器的情况看，利用率往往仅为10%。如果我们将这8个应用程序合并到一个更强大的服务器上运行，利用率可达70%，用电量为4kW，这种单一服务器的工作能效更高。此外，如果我们采用一种电源管理技术关闭前面的4个系统，也可实现系统总能耗为4kW，利用率为70%的效果。
热负荷及基础设施其他插件的功耗也同步下降。正是由于这种双重下降，使系统合并得以成为实现绿色数据中心的巨大杠杆。但是在各自的应用程序转移到合并系统期间，系统1至4不能停机。因此，在迁移过程中，会临时出现资源需求加大的问题。
（2）虚拟化：最环保的技术虚拟化是一种系统抽象化的概念。这种技术可以显著减少数据中心所需的IT设备。虚拟化消除了服务器、存储或网络设备对应用程序的物理局限。每种应用程序配置专用服务器，效率低下，造成利用率下降。虚拟化可使应用“拼车”使用服务器，这种物理意义上的车（服务器）是固定的，但乘员（应用程序）可以改变，而且变化多样（尺寸和
类型），资源自由增减。
虚拟化一词广泛使用并有以下多种定义：
◆可以生成CPU、存储器和I/O功能组成的计算机系统逻辑实例；
◆可以是其他虚拟组件的组合；
◆可组成虚拟CPU或虚拟存储器和磁盘；
◆可以是虚拟计算机与外部环境之间的虚拟网络。
为完成处理工作，虚拟系统必须在实际系统上运行，显然，这要求具有更高的智能水平。现在有纯软件解决方案，系统固件可以提供虚拟化功能，或将此类功能接入系统中。目前许多处理器架构集成了虚拟功能，可供软件解决方案使用。
虚拟化的其他优点：
◆虚拟系统支持网络虚拟化，利用虚拟化系统功能进行通信，能以极快的速度传送内存数据，提高了性能和能效，同时，减少了现场和设备资源的需求；
◆虚拟系统可以彼此实现磁盘共享。从能效角度看，通过虚拟化存储，虚拟化系统可将理想的磁盘容量提供给其他虚拟系统。
三、绿色数据中心的运行与能耗管理
3.1计算机系统用电数据收集
新的信息化系统都应该内置测量电耗的测量功能和热敏传感器。我们可利用这功能显示当前用电值，便于根据系统整体状态采取措施。智能电源分配单元（iPDUs）用于未嵌入或无可管理板载测量仪的系统。iPDU含有通用传感器，可提供连接设备的电耗信息及环境信息，如温度和湿度。iPDU的串口和LAN接口可供Web浏览器、任何基于SNMP的网管系统、Telnet或串行线连接的控制台进行远程监控和管理。事件通过SNMP陷阱或电子邮件发送通知，并可通过电子邮件发送日记录报告，能源管理器也可以管理iPDU。
3.2电源管理：硬件端一般先进的处理器系统都应具备能效管理技术，以提供多种电源管理功能：
◆用电量趋势：供计算机收集并在内部保存用电量数据，数据可通过能源管理器显示；
◆节电模式：按预定比例降低处理器电压和频率。节电模式在保证正常安全运行的同时，有助于降低峰值能耗。例如，夜间CPU利用率很低时，处理器可以采用节电模式（PowerSaverMode模式）；
◆功率封顶：强制执行用电量标定极限。这个功能适于通用电源极限条件下使用，如适用于一组系统的最大供电量。不过，这个功能不能作为节电功能使用，这样会严重影响性能；
◆处理器内核休眠：采用处理器低功率模式（称为Nap），可通过关闭内核时钟减少电耗。根据操作系统的信号，管理程序控制进入或退出Nap模式；
◆EnergyScaleforI/O：这一功能可使自动关闭PCI插拔适配器插槽的电源。如果插槽是空的，插槽电源自动关闭，不再为其分配分区，或关闭已分配分区的电源。
3.3电源管理：软件端先进的系统也应具备智能的能源管理平台，提供通用系统管理环境。
◆测量和显示被管理系统当前电量和温度数据；
◆提供选定期间内的趋势数据；
◆在固件支持的情况下，设置系统功率封顶，并管理处理器的节电模式。
图8所示应用场景展示了能源管理系统帮助优化机架或BladeCenter布局方法：
（1）假定电量趋势表明服务器实际电耗低于标定量。查找某个服务器绝不可能超出用电量。
（2）采用能源管理系统对功率进行封顶。按观测到的最大量级封顶系统功率（由于会严重影响性能，因此封顶功率设定的极限应该是正常情况下不可能达到的）。以前过度分配给每一单个系统的电量，现在可在过度分配的机架层进行管理。
（3）在机架中增加系统，以有效利用以前过度分配的电量。理想情况下，以能效更高的新系统取代低能效设备。建立电源管理基础设施后，可以采取其他步骤，例如，确定数据中心的热点位置；将应用软件配置到冷服务器上，避免热点；如果电力供应商实行负荷费率或白天和夜间费率，可以根据基准优化配置，降低能耗。
3.4集成能源和系统管理
对于整个系统管理环境，电源管理只是其中的一个方面。集成能源和系统管理系统可用于监控操作系统、数据库、服务器，直至通过灵活定制的门户监控分布式环境。
优化能效的方式包括：
◆根据机器的环境温度重新分配服务器，或将整个机架的总能耗重新分配给数据中心温度较低的另一机架。当温度报警时，可重新配置功能；
◆对存在温度问题的服务器进行功率封顶，可能是由于气流受阻造成的，直到现场问题得到解决为止；
◆将电量、温度和CPU用量数据传送到监控仓库。可按核算数据对这个数据进行修正。根据CPU和相关用电量向IT用户收费。
四、不断演进的绿色数据中心
绿色数据中心的建设不是一劳永逸的项目，而是一个不断推进的过程。从降低成本，到提高可持续发展能力，再到为企业树立更好的公众形象，整个绿色数据中心的建设过程实际是一个逐渐演进的过程。
未来，会有越来越多的公司采用EMC、能源合同管理的方式来管理数据中心的能耗，能源合同管理能帮助客户确定基准，提高IT能效水平，减少对环境的影响。同时也会出现由第三方能源评估机构对消耗的能源降低进行认证，颁发证书，可供企业用来获得降低能耗的认证测量结果。证书可在不断扩大的能效证书市场上进行有价交易，也可以保存用来证明减少能源消耗及相关的CO2排放。
参考文献
[1]赵荣义,钱以明,范存养,等.简明空调设计手册[Ｍ].北京:中国建筑工业出版社,1998:56-57.